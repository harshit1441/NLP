{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPDZMZOzn+OHHZgIH/6o4od",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshit1441/NLP/blob/main/NLP_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install libraries (run in Colab). If you're running locally, you may not need installs.\n",
        "!pip install -q datasets transformers evaluate seqeval accelerate gradio scikit-learn matplotlib torch>=1.12.0\n",
        "!pip install -U transformers accelerate\n",
        "\n",
        "\n",
        "# GPU check (optional)\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "7STNVfSMjp7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and global config\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          TrainingArguments, Trainer, DataCollatorWithPadding)\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "ATuL_Yz6jqce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load amazon_reviews_multi (multilingual) and inspect languages\n",
        "# This dataset provides reviews in multiple languages with rating (1-5) and content.\n",
        "ds = load_dataset(\"amazon_polarity\")  # fallback if amazon_reviews_multi not available\n",
        "# Note: If you prefer amazon_reviews_multi, replace above with:\n",
        "# ds = load_dataset(\"amazon_reviews_multi\", \"default\")\n",
        "\n",
        "# Quick peek\n",
        "print(ds)\n",
        "print(ds['train'][0])\n"
      ],
      "metadata": {
        "id": "IIaNJFMOj3Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Prepare dataset subsets and labels\n",
        "# For amazon_polarity labels are already binary (0: negative, 1: positive)\n",
        "# We'll create small subsets for quick experiments: train on English, test on other langs (if available).\n",
        "# If dataset doesn't contain language field, we will simulate multilingual evaluation by translating or using other sources.\n",
        "# Here, we'll just create small train/validation/test splits for demonstration.\n",
        "\n",
        "dataset = ds['train'].train_test_split(test_size=0.1, seed=seed)\n",
        "dataset = DatasetDict({\n",
        "    'train': dataset['train'].shuffle(seed=seed).select(range(20000)),  # limit size for demo\n",
        "    'test': dataset['test'].shuffle(seed=seed).select(range(5000))\n",
        "})\n",
        "print(dataset)\n",
        "print(\"Example:\", dataset['train'][0])\n"
      ],
      "metadata": {
        "id": "LxGYRSiFkg1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Light model selection + tokenizer setup (memory-friendly)\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Disable Weights & Biases tracking\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Smaller models to reduce RAM/VRAM\n",
        "model_name_mono = \"distilbert-base-uncased\"              # English only\n",
        "model_name_multi = \"distilbert-base-multilingual-cased\"  # Multilingual\n",
        "\n",
        "tokenizer_mono = AutoTokenizer.from_pretrained(model_name_mono)\n",
        "tokenizer_multi = AutoTokenizer.from_pretrained(model_name_multi)\n",
        "\n",
        "max_length = 256\n",
        "print(\"Tokenizers loaded successfully ✅\")\n"
      ],
      "metadata": {
        "id": "SmdVjLTAksIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Tokenize and keep 'label' column (avoid memory overflow)\n",
        "from datasets import DatasetDict\n",
        "\n",
        "# --- Limit dataset size for Colab (adjust numbers if you have more RAM) ---\n",
        "dataset = DatasetDict({\n",
        "    \"train\": dataset[\"train\"].shuffle(seed=42).select(range(2000)),\n",
        "    \"test\": dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "})\n",
        "\n",
        "def tokenize_mono(examples):\n",
        "    return tokenizer_mono(examples[\"content\"], truncation=True, max_length=max_length)\n",
        "\n",
        "def tokenize_multi(examples):\n",
        "    return tokenizer_multi(examples[\"content\"], truncation=True, max_length=max_length)\n",
        "\n",
        "tokenized_mono = dataset.map(tokenize_mono, batched=True)\n",
        "tokenized_multi = dataset.map(tokenize_multi, batched=True)\n",
        "\n",
        "# Remove unused columns to free RAM\n",
        "tokenized_mono = tokenized_mono.remove_columns(\n",
        "    [c for c in tokenized_mono[\"train\"].column_names if c not in [\"input_ids\",\"attention_mask\",\"label\"]]\n",
        ")\n",
        "tokenized_multi = tokenized_multi.remove_columns(\n",
        "    [c for c in tokenized_multi[\"train\"].column_names if c not in [\"input_ids\",\"attention_mask\",\"label\"]]\n",
        ")\n",
        "\n",
        "print(\"✅ Tokenization done\")\n",
        "print(\"Columns:\", tokenized_mono[\"train\"].column_names)\n"
      ],
      "metadata": {
        "id": "Zpq2dcMokwZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 (final, tested on transformers 4.40+)\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np, torch, os\n",
        "\n",
        "def make_model_and_trainer(model_name, tokenized_dataset, tokenizer, output_dir,\n",
        "                           epochs=1, batch_size=8, learning_rate=2e-5):\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "    num_labels = 2\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=num_labels\n",
        "    ).to(device)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"epoch\", # Changed from evaluation_strategy\n",
        "        save_strategy=\"no\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        gradient_checkpointing=True,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=\"none\",          # disable wandb/tensorboard\n",
        "        logging_dir=f\"{output_dir}/logs\"\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "    def compute_metrics(pred):\n",
        "        labels = pred.label_ids\n",
        "        preds = np.argmax(pred.predictions, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
        "        }\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    return model, trainer"
      ],
      "metadata": {
        "id": "l0XqKkeHkz7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mono_out = \"runs/distilbert_fastdemo\"\n",
        "\n",
        "model_mono, trainer_mono = make_model_and_trainer(\n",
        "    model_name_mono, tokenized_mono, tokenizer_mono,\n",
        "    output_dir=mono_out, epochs=0.5, batch_size=8\n",
        ")\n",
        "\n",
        "# Override trainer args for speed\n",
        "trainer_mono.args.evaluation_strategy = \"no\"\n",
        "trainer_mono.args.logging_steps = 200\n",
        "trainer_mono.args.save_strategy = \"no\"\n",
        "trainer_mono.args.gradient_accumulation_steps = 4\n",
        "trainer_mono.args.per_device_train_batch_size = 2\n",
        "trainer_mono.args.per_device_eval_batch_size = 2\n",
        "\n",
        "trainer_mono.train()\n",
        "print(\"✅ Training finished (fast demo mode)\")\n"
      ],
      "metadata": {
        "id": "cyAt2YS6k7LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Train multilingual model (XLM-R) on same English training data\n",
        "multi_out = \"runs/xlm_roberta_multi_demo\"\n",
        "model_multi, trainer_multi = make_model_and_trainer(model_name_multi, tokenized_multi, tokenizer_multi, multi_out, epochs=0.5, batch_size=16, learning_rate=2e-5)\n",
        "# Override trainer args for speed\n",
        "trainer_mono.args.evaluation_strategy = \"no\"\n",
        "trainer_mono.args.logging_steps = 200\n",
        "trainer_mono.args.save_strategy = \"no\"\n",
        "trainer_mono.args.gradient_accumulation_steps = 4\n",
        "trainer_mono.args.per_device_train_batch_size = 2\n",
        "trainer_mono.args.per_device_eval_batch_size = 2\n",
        "trainer_multi.train()\n",
        "eval_multi = trainer_multi.evaluate()\n",
        "print(\"Multilingual eval:\", eval_multi)\n"
      ],
      "metadata": {
        "id": "ywEB_oU9m4kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Evaluate and produce predictions + classification report\n",
        "def get_preds_and_report(trainer, dataset, tokenizer):\n",
        "    preds_output = trainer.predict(dataset)\n",
        "    preds = np.argmax(preds_output.predictions, axis=1)\n",
        "    labels = preds_output.label_ids\n",
        "    print(classification_report(labels, preds, digits=4))\n",
        "    return preds, labels\n",
        "\n",
        "print(\"Monolingual model report:\")\n",
        "mono_preds, mono_labels = get_preds_and_report(trainer_mono, tokenized_mono['test'], tokenizer_mono)\n",
        "\n",
        "print(\"Multilingual model report:\")\n",
        "multi_preds, multi_labels = get_preds_and_report(trainer_multi, tokenized_multi['test'], tokenizer_multi)\n",
        "# Cell 10: Evaluate and produce predictions + classification report\n",
        "def get_preds_and_report(trainer, dataset, tokenizer):\n",
        "    preds_output = trainer.predict(dataset)\n",
        "    preds = np.argmax(preds_output.predictions, axis=1)\n",
        "    labels = preds_output.label_ids\n",
        "    print(classification_report(labels, preds, digits=4))\n",
        "    return preds, labels\n",
        "\n",
        "print(\"Monolingual model report:\")\n",
        "mono_preds, mono_labels = get_preds_and_report(trainer_mono, tokenized_mono['test'], tokenizer_mono)\n",
        "\n",
        "print(\"Multilingual model report:\")\n",
        "multi_preds, multi_labels = get_preds_and_report(trainer_multi, tokenized_multi['test'], tokenizer_multi)\n"
      ],
      "metadata": {
        "id": "oNb6lGZq6Fvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Simple visualizations\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Accuracy and F1 we got from Trainer evaluations\n",
        "mono_acc = eval_mono.get(\"eval_accuracy\", None)\n",
        "mono_f1 = eval_mono.get(\"eval_f1\", None)\n",
        "multi_acc = eval_multi.get(\"eval_accuracy\", None)\n",
        "multi_f1 = eval_multi.get(\"eval_f1\", None)\n",
        "\n",
        "# Bar chart comparing accuracy/f1\n",
        "labels = [\"Monolingual (BERT)\", \"Multilingual (XLM-R)\"]\n",
        "accs = [mono_acc or 0, multi_acc or 0]\n",
        "f1s = [mono_f1 or 0, multi_f1 or 0]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,4))\n",
        "ax.bar(x - width/2, accs, width, label='Accuracy')\n",
        "ax.bar(x + width/2, f1s, width, label='F1 (weighted)')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, rotation=20)\n",
        "ax.set_ylim(0,1)\n",
        "ax.set_ylabel(\"Score\")\n",
        "ax.set_title(\"Model comparison (quick demo)\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix for multilingual model\n",
        "cm_multi = confusion_matrix(multi_labels, multi_preds)\n",
        "disp = ConfusionMatrixDisplay(cm_multi, display_labels=[\"neg\",\"pos\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix - Multilingual Model\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j7QZYmEO6Huk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Show examples where the models disagree\n",
        "def decode_input(tokenizer, tokenized_batch, idx):\n",
        "    # reconstruct input text if original text removed\n",
        "    # if we have no original text column, we can't easily decode; show tokens instead\n",
        "    input_ids = tokenized_batch['input_ids'][idx]\n",
        "    return tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "\n",
        "# We'll compare predictions on the first 200 test examples\n",
        "disagreements = []\n",
        "for i in range(min(200, len(tokenized_mono['test']))):\n",
        "    # get predictions by running models directly in eval mode\n",
        "    with torch.no_grad():\n",
        "        # mono\n",
        "        mi = {k: torch.tensor([v[i]]).to(device) for k,v in tokenized_mono['test'][i].items() if k in ['input_ids','attention_mask','token_type_ids'] or k in ['input_ids','attention_mask']}\n",
        "        mo_logits = model_mono(**{k:v for k,v in mi.items() if k in model_mono.forward.__code__.co_varnames})\n",
        "        m_pred = int(torch.argmax(mo_logits.logits, dim=1).cpu().numpy())\n",
        "\n",
        "        # multi\n",
        "        xi = {k: torch.tensor([v[i]]).to(device) for k,v in tokenized_multi['test'][i].items() if k in ['input_ids','attention_mask']}\n",
        "        xo_logits = model_multi(**{k:v for k,v in xi.items() if k in model_multi.forward.__code__.co_varnames})\n",
        "        x_pred = int(torch.argmax(xo_logits.logits, dim=1).cpu().numpy())\n",
        "\n",
        "    true_label = tokenized_mono['test'][i]['label']\n",
        "    if m_pred != x_pred:\n",
        "        text = decode_input(tokenizer_mono, tokenized_mono['test'], i)\n",
        "        disagreements.append((i, text, true_label, m_pred, x_pred))\n",
        "        if len(disagreements) >= 10:\n",
        "            break\n",
        "\n",
        "for d in disagreements:\n",
        "    idx, text, true, mono_p, multi_p = d\n",
        "    print(f\"Idx {idx} | True: {true} | Mono: {mono_p} | Multi: {multi_p}\\n{text}\\n---\\n\")\n"
      ],
      "metadata": {
        "id": "zXPH1vJm6Jf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Gradio demo to try both models interactively\n",
        "import gradio as gr\n",
        "\n",
        "# load tokenizer and models on CPU for demo if GPU not available\n",
        "mono_pipeline_tokenizer = tokenizer_mono\n",
        "multi_pipeline_tokenizer = tokenizer_multi\n",
        "model_mono.eval()\n",
        "model_multi.eval()\n",
        "\n",
        "def predict_both(text):\n",
        "    # mono\n",
        "    tok_m = mono_pipeline_tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\", max_length=max_length).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits_m = model_mono(**tok_m).logits\n",
        "    pred_m = int(torch.argmax(logits_m, dim=1).cpu().numpy())\n",
        "    # multi\n",
        "    tok_x = multi_pipeline_tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\", max_length=max_length).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits_x = model_multi(**tok_x).logits\n",
        "    pred_x = int(torch.argmax(logits_x, dim=1).cpu().numpy())\n",
        "    label_map = {0:\"Negative\", 1:\"Positive\"}\n",
        "    return label_map[pred_m], label_map[pred_x]\n",
        "\n",
        "iface = gr.Interface(fn=predict_both,\n",
        "                     inputs=gr.Textbox(lines=4, placeholder=\"Enter review here...\"),\n",
        "                     outputs=[gr.Label(num_top_classes=1, label=\"Monolingual (BERT)\"),\n",
        "                              gr.Label(num_top_classes=1, label=\"Multilingual (XLM-R)\")],\n",
        "                     title=\"Monolingual vs Multilingual Sentiment Demo\",\n",
        "                     description=\"Enter text (any language). Models trained on English demo subset; multilingual model may generalize better to other languages.\")\n",
        "iface.launch(share=False)\n"
      ],
      "metadata": {
        "id": "XQDgGCXz6Lz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Save the fine-tuned models & tokenizers locally\n",
        "save_dir_mono = \"saved_models/bert_mono\"\n",
        "save_dir_multi = \"saved_models/xlm_roberta_multi\"\n",
        "os.makedirs(save_dir_mono, exist_ok=True)\n",
        "os.makedirs(save_dir_multi, exist_ok=True)\n",
        "\n",
        "model_mono.save_pretrained(save_dir_mono)\n",
        "tokenizer_mono.save_pretrained(save_dir_mono)\n",
        "\n",
        "model_multi.save_pretrained(save_dir_multi)\n",
        "tokenizer_multi.save_pretrained(save_dir_multi)\n",
        "\n",
        "print(\"Saved models to:\", save_dir_mono, save_dir_multi)\n"
      ],
      "metadata": {
        "id": "re8dO1fK6N6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t5dCNFQq6P7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}