{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdTmaS1ov0krJ5j6FV9sH4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshit1441/NLP/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install libraries (run in Colab). If you're running locally, you may not need installs.\n",
        "!pip install -q datasets transformers evaluate seqeval accelerate gradio scikit-learn matplotlib torch>=1.12.0\n",
        "!pip install -U transformers accelerate\n",
        "\n",
        "\n",
        "# GPU check (optional)\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "7STNVfSMjp7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b71798-6930-4aa0-bd60-f07004c48287"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and global config\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          TrainingArguments, Trainer, DataCollatorWithPadding)\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "ATuL_Yz6jqce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4928dbba-c487-4e37-e2ab-11c1598c7dc1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load amazon_reviews_multi (multilingual) and inspect languages\n",
        "# This dataset provides reviews in multiple languages with rating (1-5) and content.\n",
        "ds = load_dataset(\"amazon_polarity\")  # fallback if amazon_reviews_multi not available\n",
        "# Note: If you prefer amazon_reviews_multi, replace above with:\n",
        "# ds = load_dataset(\"amazon_reviews_multi\", \"default\")\n",
        "\n",
        "# Quick peek\n",
        "print(ds)\n",
        "print(ds['train'][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIaNJFMOj3Oo",
        "outputId": "c948ad96-86fd-4c84-b3d5-8d2d78e37867"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'title', 'content'],\n",
            "        num_rows: 3600000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'title', 'content'],\n",
            "        num_rows: 400000\n",
            "    })\n",
            "})\n",
            "{'label': 1, 'title': 'Stuning even for the non-gamer', 'content': 'This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Prepare dataset subsets and labels\n",
        "# For amazon_polarity labels are already binary (0: negative, 1: positive)\n",
        "# We'll create small subsets for quick experiments: train on English, test on other langs (if available).\n",
        "# If dataset doesn't contain language field, we will simulate multilingual evaluation by translating or using other sources.\n",
        "# Here, we'll just create small train/validation/test splits for demonstration.\n",
        "\n",
        "dataset = ds['train'].train_test_split(test_size=0.1, seed=seed)\n",
        "dataset = DatasetDict({\n",
        "    'train': dataset['train'].shuffle(seed=seed).select(range(20000)),  # limit size for demo\n",
        "    'test': dataset['test'].shuffle(seed=seed).select(range(5000))\n",
        "})\n",
        "print(dataset)\n",
        "print(\"Example:\", dataset['train'][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxGYRSiFkg1n",
        "outputId": "b47d3635-5d30-43cb-a244-178caa4c8b7c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'title', 'content'],\n",
            "        num_rows: 20000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'title', 'content'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "})\n",
            "Example: {'label': 0, 'title': 'We thought the first one was a lemon ...', 'content': \"but when the 2nd one (which was very efficiently sent by Amazon as a replacement for the 1st one that burned out within a month) lasted only 3 weeks it became a trend. Directions included with popper were followed so it wasn't anything we did. Some customers have been happy with this product but that has not been our experience. Will not be buying again.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Light model selection + tokenizer setup (memory-friendly)\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Disable Weights & Biases tracking\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Smaller models to reduce RAM/VRAM\n",
        "model_name_mono = \"distilbert-base-uncased\"              # English only\n",
        "model_name_multi = \"distilbert-base-multilingual-cased\"  # Multilingual\n",
        "\n",
        "tokenizer_mono = AutoTokenizer.from_pretrained(model_name_mono)\n",
        "tokenizer_multi = AutoTokenizer.from_pretrained(model_name_multi)\n",
        "\n",
        "max_length = 256\n",
        "print(\"Tokenizers loaded successfully ✅\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmdVjLTAksIA",
        "outputId": "a4d0607e-9986-4bd5-86d3-97a74a9dbf3d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizers loaded successfully ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Tokenize and keep 'label' column (avoid memory overflow)\n",
        "from datasets import DatasetDict\n",
        "\n",
        "# --- Limit dataset size for Colab (adjust numbers if you have more RAM) ---\n",
        "dataset = DatasetDict({\n",
        "    \"train\": dataset[\"train\"].shuffle(seed=42).select(range(2000)),\n",
        "    \"test\": dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "})\n",
        "\n",
        "def tokenize_mono(examples):\n",
        "    return tokenizer_mono(examples[\"content\"], truncation=True, max_length=max_length)\n",
        "\n",
        "def tokenize_multi(examples):\n",
        "    return tokenizer_multi(examples[\"content\"], truncation=True, max_length=max_length)\n",
        "\n",
        "tokenized_mono = dataset.map(tokenize_mono, batched=True)\n",
        "tokenized_multi = dataset.map(tokenize_multi, batched=True)\n",
        "\n",
        "# Remove unused columns to free RAM\n",
        "tokenized_mono = tokenized_mono.remove_columns(\n",
        "    [c for c in tokenized_mono[\"train\"].column_names if c not in [\"input_ids\",\"attention_mask\",\"label\"]]\n",
        ")\n",
        "tokenized_multi = tokenized_multi.remove_columns(\n",
        "    [c for c in tokenized_multi[\"train\"].column_names if c not in [\"input_ids\",\"attention_mask\",\"label\"]]\n",
        ")\n",
        "\n",
        "print(\"✅ Tokenization done\")\n",
        "print(\"Columns:\", tokenized_mono[\"train\"].column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpq2dcMokwZ3",
        "outputId": "ecc6fd79-5ad7-4d29-83a6-9d1e4b24edf2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tokenization done\n",
            "Columns: ['label', 'input_ids', 'attention_mask']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 (final, tested on transformers 4.40+)\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np, torch, os\n",
        "\n",
        "def make_model_and_trainer(model_name, tokenized_dataset, tokenizer, output_dir,\n",
        "                           epochs=1, batch_size=8, learning_rate=2e-5):\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "    num_labels = 2\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=num_labels\n",
        "    ).to(device)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"epoch\", # Changed from evaluation_strategy\n",
        "        save_strategy=\"no\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        gradient_checkpointing=True,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=\"none\",          # disable wandb/tensorboard\n",
        "        logging_dir=f\"{output_dir}/logs\"\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "    def compute_metrics(pred):\n",
        "        labels = pred.label_ids\n",
        "        preds = np.argmax(pred.predictions, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
        "        }\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    return model, trainer"
      ],
      "metadata": {
        "id": "l0XqKkeHkz7H"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mono_out = \"runs/distilbert_fastdemo\"\n",
        "\n",
        "model_mono, trainer_mono = make_model_and_trainer(\n",
        "    model_name_mono, tokenized_mono, tokenizer_mono,\n",
        "    output_dir=mono_out, epochs=0.5, batch_size=8\n",
        ")\n",
        "\n",
        "# Override trainer args for speed\n",
        "trainer_mono.args.evaluation_strategy = \"no\"\n",
        "trainer_mono.args.logging_steps = 200\n",
        "trainer_mono.args.save_strategy = \"no\"\n",
        "trainer_mono.args.gradient_accumulation_steps = 4\n",
        "trainer_mono.args.per_device_train_batch_size = 2\n",
        "trainer_mono.args.per_device_eval_batch_size = 2\n",
        "\n",
        "trainer_mono.train()\n",
        "print(\"✅ Training finished (fast demo mode)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "cyAt2YS6k7LO",
        "outputId": "d9a152ed-ce4d-465e-eb0c-409a568e67cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3292898321.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 22/125 03:23 < 17:26, 0.10 it/s, Epoch 0.08/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Train multilingual model (XLM-R) on same English training data\n",
        "multi_out = \"runs/xlm_roberta_multi_demo\"\n",
        "model_multi, trainer_multi = make_model_and_trainer(model_name_multi, tokenized_multi, tokenizer_multi, multi_out, epochs=1, batch_size=16, learning_rate=2e-5)\n",
        "trainer_multi.train()\n",
        "eval_multi = trainer_multi.evaluate()\n",
        "print(\"Multilingual eval:\", eval_multi)\n"
      ],
      "metadata": {
        "id": "ywEB_oU9m4kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Evaluate and produce predictions + classification report\n",
        "def get_preds_and_report(trainer, dataset, tokenizer):\n",
        "    preds_output = trainer.predict(dataset)\n",
        "    preds = np.argmax(preds_output.predictions, axis=1)\n",
        "    labels = preds_output.label_ids\n",
        "    print(classification_report(labels, preds, digits=4))\n",
        "    return preds, labels\n",
        "\n",
        "print(\"Monolingual model report:\")\n",
        "mono_preds, mono_labels = get_preds_and_report(trainer_mono, tokenized_mono['test'], tokenizer_mono)\n",
        "\n",
        "print(\"Multilingual model report:\")\n",
        "multi_preds, multi_labels = get_preds_and_report(trainer_multi, tokenized_multi['test'], tokenizer_multi)\n",
        "# Cell 10: Evaluate and produce predictions + classification report\n",
        "def get_preds_and_report(trainer, dataset, tokenizer):\n",
        "    preds_output = trainer.predict(dataset)\n",
        "    preds = np.argmax(preds_output.predictions, axis=1)\n",
        "    labels = preds_output.label_ids\n",
        "    print(classification_report(labels, preds, digits=4))\n",
        "    return preds, labels\n",
        "\n",
        "print(\"Monolingual model report:\")\n",
        "mono_preds, mono_labels = get_preds_and_report(trainer_mono, tokenized_mono['test'], tokenizer_mono)\n",
        "\n",
        "print(\"Multilingual model report:\")\n",
        "multi_preds, multi_labels = get_preds_and_report(trainer_multi, tokenized_multi['test'], tokenizer_multi)\n"
      ],
      "metadata": {
        "id": "oNb6lGZq6Fvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Simple visualizations\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Accuracy and F1 we got from Trainer evaluations\n",
        "mono_acc = eval_mono.get(\"eval_accuracy\", None)\n",
        "mono_f1 = eval_mono.get(\"eval_f1\", None)\n",
        "multi_acc = eval_multi.get(\"eval_accuracy\", None)\n",
        "multi_f1 = eval_multi.get(\"eval_f1\", None)\n",
        "\n",
        "# Bar chart comparing accuracy/f1\n",
        "labels = [\"Monolingual (BERT)\", \"Multilingual (XLM-R)\"]\n",
        "accs = [mono_acc or 0, multi_acc or 0]\n",
        "f1s = [mono_f1 or 0, multi_f1 or 0]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,4))\n",
        "ax.bar(x - width/2, accs, width, label='Accuracy')\n",
        "ax.bar(x + width/2, f1s, width, label='F1 (weighted)')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, rotation=20)\n",
        "ax.set_ylim(0,1)\n",
        "ax.set_ylabel(\"Score\")\n",
        "ax.set_title(\"Model comparison (quick demo)\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix for multilingual model\n",
        "cm_multi = confusion_matrix(multi_labels, multi_preds)\n",
        "disp = ConfusionMatrixDisplay(cm_multi, display_labels=[\"neg\",\"pos\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix - Multilingual Model\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j7QZYmEO6Huk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Show examples where the models disagree\n",
        "def decode_input(tokenizer, tokenized_batch, idx):\n",
        "    # reconstruct input text if original text removed\n",
        "    # if we have no original text column, we can't easily decode; show tokens instead\n",
        "    input_ids = tokenized_batch['input_ids'][idx]\n",
        "    return tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "\n",
        "# We'll compare predictions on the first 200 test examples\n",
        "disagreements = []\n",
        "for i in range(min(200, len(tokenized_mono['test']))):\n",
        "    # get predictions by running models directly in eval mode\n",
        "    with torch.no_grad():\n",
        "        # mono\n",
        "        mi = {k: torch.tensor([v[i]]).to(device) for k,v in tokenized_mono['test'][i].items() if k in ['input_ids','attention_mask','token_type_ids'] or k in ['input_ids','attention_mask']}\n",
        "        mo_logits = model_mono(**{k:v for k,v in mi.items() if k in model_mono.forward.__code__.co_varnames})\n",
        "        m_pred = int(torch.argmax(mo_logits.logits, dim=1).cpu().numpy())\n",
        "\n",
        "        # multi\n",
        "        xi = {k: torch.tensor([v[i]]).to(device) for k,v in tokenized_multi['test'][i].items() if k in ['input_ids','attention_mask']}\n",
        "        xo_logits = model_multi(**{k:v for k,v in xi.items() if k in model_multi.forward.__code__.co_varnames})\n",
        "        x_pred = int(torch.argmax(xo_logits.logits, dim=1).cpu().numpy())\n",
        "\n",
        "    true_label = tokenized_mono['test'][i]['label']\n",
        "    if m_pred != x_pred:\n",
        "        text = decode_input(tokenizer_mono, tokenized_mono['test'], i)\n",
        "        disagreements.append((i, text, true_label, m_pred, x_pred))\n",
        "        if len(disagreements) >= 10:\n",
        "            break\n",
        "\n",
        "for d in disagreements:\n",
        "    idx, text, true, mono_p, multi_p = d\n",
        "    print(f\"Idx {idx} | True: {true} | Mono: {mono_p} | Multi: {multi_p}\\n{text}\\n---\\n\")\n"
      ],
      "metadata": {
        "id": "zXPH1vJm6Jf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Gradio demo to try both models interactively\n",
        "import gradio as gr\n",
        "\n",
        "# load tokenizer and models on CPU for demo if GPU not available\n",
        "mono_pipeline_tokenizer = tokenizer_mono\n",
        "multi_pipeline_tokenizer = tokenizer_multi\n",
        "model_mono.eval()\n",
        "model_multi.eval()\n",
        "\n",
        "def predict_both(text):\n",
        "    # mono\n",
        "    tok_m = mono_pipeline_tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\", max_length=max_length).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits_m = model_mono(**tok_m).logits\n",
        "    pred_m = int(torch.argmax(logits_m, dim=1).cpu().numpy())\n",
        "    # multi\n",
        "    tok_x = multi_pipeline_tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\", max_length=max_length).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits_x = model_multi(**tok_x).logits\n",
        "    pred_x = int(torch.argmax(logits_x, dim=1).cpu().numpy())\n",
        "    label_map = {0:\"Negative\", 1:\"Positive\"}\n",
        "    return label_map[pred_m], label_map[pred_x]\n",
        "\n",
        "iface = gr.Interface(fn=predict_both,\n",
        "                     inputs=gr.Textbox(lines=4, placeholder=\"Enter review here...\"),\n",
        "                     outputs=[gr.Label(num_top_classes=1, label=\"Monolingual (BERT)\"),\n",
        "                              gr.Label(num_top_classes=1, label=\"Multilingual (XLM-R)\")],\n",
        "                     title=\"Monolingual vs Multilingual Sentiment Demo\",\n",
        "                     description=\"Enter text (any language). Models trained on English demo subset; multilingual model may generalize better to other languages.\")\n",
        "iface.launch(share=False)\n"
      ],
      "metadata": {
        "id": "XQDgGCXz6Lz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Save the fine-tuned models & tokenizers locally\n",
        "save_dir_mono = \"saved_models/bert_mono\"\n",
        "save_dir_multi = \"saved_models/xlm_roberta_multi\"\n",
        "os.makedirs(save_dir_mono, exist_ok=True)\n",
        "os.makedirs(save_dir_multi, exist_ok=True)\n",
        "\n",
        "model_mono.save_pretrained(save_dir_mono)\n",
        "tokenizer_mono.save_pretrained(save_dir_mono)\n",
        "\n",
        "model_multi.save_pretrained(save_dir_multi)\n",
        "tokenizer_multi.save_pretrained(save_dir_multi)\n",
        "\n",
        "print(\"Saved models to:\", save_dir_mono, save_dir_multi)\n"
      ],
      "metadata": {
        "id": "re8dO1fK6N6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t5dCNFQq6P7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}